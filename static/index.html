<!DOCTYPE html>
<html lang="en">
<head>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <title>Deep-Dive Research Agent</title>
    <style>
        :root { --primary: #00f2ea; --bg: #0f172a; --card: #1e293b; --text: #f8fafc; }
        body { font-family: 'Segoe UI', sans-serif; background-color: var(--bg); color: var(--text); display: flex; flex-direction: column; align-items: center; justify-content: center; height: 100vh; margin: 0; }
        
        /* The Main Card */
        .container { background: var(--card); padding: 40px; border-radius: 20px; box-shadow: 0 10px 30px rgba(0,0,0,0.5); text-align: center; width: 90%; max-width: 500px; border: 1px solid #334155; }
        
        h1 { margin-bottom: 10px; font-size: 24px; background: linear-gradient(90deg, #00f2ea, #ff0050); -webkit-background-clip: text; -webkit-text-fill-color: transparent; }
        p { color: #94a3b8; margin-bottom: 30px; }

        /* The Button */
        button {
            background: linear-gradient(135deg, #00f2ea 0%, #007bff 100%);
            border: none; padding: 18px 40px; color: #000; font-weight: bold; font-size: 18px; border-radius: 50px; cursor: pointer; transition: transform 0.2s, box-shadow 0.2s;
            box-shadow: 0 0 20px rgba(0, 242, 234, 0.4);
        }
        button:hover { transform: scale(1.05); box-shadow: 0 0 30px rgba(0, 242, 234, 0.6); }
        button:disabled { background: #334155; color: #64748b; box-shadow: none; cursor: not-allowed; transform: scale(1); }

        /* Status & Response Areas */
        #status { margin-top: 25px; font-weight: 600; color: var(--primary); height: 24px; }
        #response-area { 
            margin-top: 20px; text-align: left; background: #0b1120; padding: 20px; border-radius: 10px; border-left: 4px solid var(--primary); 
            min-height: 80px; font-size: 1.1em; line-height: 1.6; opacity: 0; transition: opacity 0.5s;
        }
        #response-area.visible { opacity: 1; }

        /* Pulsing Animation for Recording */
        @keyframes pulse { 0% { box-shadow: 0 0 0 0 rgba(255, 0, 80, 0.7); } 70% { box-shadow: 0 0 0 15px rgba(255, 0, 80, 0); } 100% { box-shadow: 0 0 0 0 rgba(255, 0, 80, 0); } }
        .recording { animation: pulse 1.5s infinite; background: linear-gradient(135deg, #ff0050 0%, #cc0040 100%) !important; color: white !important; }
    </style>
</head>
<body>

    <div class="container">
        <h1>üéôÔ∏è Deep-Dive Agent</h1>
        <p>Autonomous Research System v1.0</p>
        
        <button id="recordBtn">Tap to Speak</button>
        <div id="status">Ready</div>
        <div id="response-area"></div>
    </div>

    <script>
        const recordBtn = document.getElementById('recordBtn');
        const status = document.getElementById('status');
        const responseArea = document.getElementById('response-area');
        let recognition;

        if (window.SpeechRecognition || window.webkitSpeechRecognition) {
            const SpeechRecognition = window.SpeechRecognition || window.webkitSpeechRecognition;
            recognition = new SpeechRecognition();
            recognition.lang = 'en-US';
            recognition.interimResults = false;

            recognition.onstart = () => {
                recordBtn.classList.add('recording');
                recordBtn.innerText = "Listening...";
                status.innerText = "Listening to your voice...";
            };

            recognition.onend = () => {
                recordBtn.classList.remove('recording');
                recordBtn.disabled = true;
                recordBtn.innerText = "Processing...";
            };

            recognition.onresult = async (event) => {
                const transcript = event.results[0][0].transcript;
                
                // UX:
                simulateThinkingProcess(transcript);

                try {
                    const response = await fetch("http://127.0.0.1:8000/process-voice", {
                        method: "POST",
                        headers: { "Content-Type": "application/json" },
                        body: JSON.stringify({ text: transcript })
                    });
                    
                    const data = await response.json();
                    
                    status.innerText = "Complete";
                    responseArea.innerText = data.reply;
                    responseArea.classList.add('visible');
                    speakResponse(data.reply);
                    
                } catch (error) {
                    console.error("Error:", error);
                    status.innerText = "Error: Backend Offline";
                }
                recordBtn.disabled = false;
                recordBtn.innerText = "Tap to Speak";
            };
        } else {
            alert("Browser not supported. Please use Chrome/Edge.");
        }


        function simulateThinkingProcess(query) {
            responseArea.classList.remove('visible');
            const steps = [
                `User said: "${query}"`,
                "üîé Searching Wikipedia & ArXiv...",
                "üß† Reading Search Results...",
                "ü§ñ Synthesizing Answer..."
            ];
            
            let i = 0;
            const interval = setInterval(() => {
                if (i < steps.length) {
                    status.innerText = steps[i];
                    i++;
                } else {
                    clearInterval(interval);
                }
            }, 800);
        }

        function speakResponse(text) {
            const utterance = new SpeechSynthesisUtterance(text);
            utterance.rate = 1.0; 
            window.speechSynthesis.speak(utterance);
        }
    </script>
</body>
</html>